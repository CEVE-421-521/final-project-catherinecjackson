---
title: "Final Project Report"
author: "Catherine Jackson (ccj3)"
jupyter: julia-1.10
date: 2024-04-30

format: 
    pdf:
        documentclass: article
        fontsize: 11pt
        geometry:
            - margin=1in  
        number-sections: true
        code-line-numbers: true

date-format: "ddd., MMM. D"

bibliography: references.bib

execute: 
  cache: false
---

# Introduction

## Storm Surge Variability and Literature Review

Hurricanes represent an ever-present threat to US coastlines, and these events have incredible potential to damage infrastructure and threaten lives.  In fact, @ncei2024 found that, when analyzing data from 1980 to 2024, damages from tropical cyclones and severe storms account for 51.8% and 17.3% of climate disaster-related costs respectively.  These severe storms and their associated surge events must be better understood in order to protect communities from their damages.

However, both the frequency and characteristics of these storms and their associated surges is made more complex due to spatial dependencies.  @Needham2012 sought to create a storm surge database based on historical activity.  They argue that this database can be used by communities to better protect themselves based on historical data.  They found that the Central and Western Gulf Coast is particularly vulnerable to hurricanes, both in terms of increased frequency and magnitude, as well as storm surge.  Comparatively, the East Coast experienced less frequent and smaller events.  Furthermore, @Xu2010 hoped to better understand how the incorporation of multi-scale simulations affected surge predictions, but their results also demonstrated that differences in coastal topography and bathymetry led to large variations in storm surge events. @islam2021new found similar results. After including information about translational speed and coastal geometry, the predictions from their surge index became significantly more accurate.  In this way, @Needham2012 demonstrates that there are significant variations along the US coastline based on hurricane magnitude and frequency, and the associated storm surge, due to important climate variations.  In addition, @Xu2010 and @islam2021new demonstrated that including information about coastal geometries and bathymetries is important to accurately modeling and predicting surge.  

The image below, from @Needham2012, visually shows the differences in storm surge across the Southern US coastline based on historical information and data.

![](Needham2012Figure1.PNG)

In conclusion, these papers all demonstrate that physical differences in the coast as well as climate differences in the storms themselves mean that surge behavior is incredibly spatial.  To accurately protect communities and analyze risk, the spatial trends must be understood and regional data must be included.

## Problem Statement

In the original risk analysis code, a representative storm surge is randomly generated.  This storm surge is represented with a GEV distribution defined by three parameters, each being generated from its own distribution.  Most importantly, though, this function was used to apply regardless of the site of interest.  The same function was used at many different points across the coastline.  As discussed above, however, it is important to include spatial variability given that severe storms and associated surges are highly dependent on the location along the coast and nearby geometry and bathymetry.  

**The problem that must be addressed, then, is a methodology by which spatial information can be incorporated into the surge function such that the surge is more accurate given the physical characteristics and location of the site.**

It is important to address this lack of spatial specificity in the current storm surge generation, within the context of climate risk analysis, as, even if the climate risk assessment methodology is appropriate and efficient, it is dependent on the quality of incoming data.  For example, a project could have a statistically sound methodology of determining whether to raise a house given certain flood risks, but if the storm surge distribution used is incorrectly low, the model would incorrectly underestimate the true risk.  Improving data inputs improves the decisions produced by this risk analysis tool.

For these reasons, the surge distribution should be as accurate as possible for the specific site of interest.  The best methodology to do so is to adjust this distribution towards the observational data from the nearest gage.

## Selected Feature

To address the spatial trends demonstrated in severe storms and associated storm surge, the surge distribution used in risk analysis must be specific to the site of interest.  Therefore, the new addition to the decision-support tool must shift the distribution such that it better represents observed data at the nearest gage.

Consequently, this selected feature will be a modification of the code that provides a representative storm surge distribution sample.  Note, first, that this code is found in lab code rather than in the HouseElevation.jl file.  Consequently, these adjustments are made within the definition of the function itself.

When selecting the best methodology by which the representative distribution can be shifted such that it better represents the behavior at the specific site of interest, I took inspiration from Dr. Doss-Gollin's CEVE 543 class.  Bayesian updating allows a distribution to be adjusted as new, observed data is produced.  This means that the original surge distribution parameters can be updated using data that comes from the nearest gage, producing distributions more accurate given the physical location of the site.

Again, this improves the climate risk assessment by using Bayesian updating as a methodology to improve the input data.  A model is only as reliable as the data it uses to make predictions, so improving the surge based on the actual physical location improves the decision outputs of this model.

# Methodology

Inspiration for this methodology is drawn primarily from @ceve543_2023. Slides from this course help describe the general process of Bayesian Updating.  First, a project might have an original distribution, defined by parameters, each with their own probability distributions.  In other words, a parameters might be defined by a Normal distribution (e.g. mean of 5 and standard deviation of 0.5), but it is not a point value.  Furthermore, the project might have a reasonable first guess as to what these distributions could be.  However, as new data is produced, this first guess can be updated so they are more accurate.  

The slides also define Bayes' rule for distributions as:

$$
p(\theta | y) \propto p(y | \theta) p(\theta)
$$

Where:

1. $p(\theta | y)$ is the posterior distribution of the parameters given the data

2. $p(y | \theta)$ is the likelihood of the data given the parameters

3. $p(\theta)$ is the prior distribution of the parameters

In other words, the prior is the best guess before any additional data is produced.  The likelihood tells us how likely the new observational data is given our beliefs within the prior.  We can use the likelihood to update our new beliefs about the distribution.

However, explicitly calculating the posterior can be computationally challenging. Therefore, many models use sampling to reduce this computational load.  This concept can be difficult to conceptualize.  One analogy is to think of the posterior as a mountainous landscape.  Higher peaks represent more likely values of the parameters.  The goal is then to find the highest peak. Instead of solving analytically, the model can "walk around" the landscape to find the best values.

Markov Chain Monte Carlo includes many different methods to walk around the landscape, but in general the process is as follows:

1. Start at a random point in the parameter space

2. Propose a new point in the parameter space

3. Determine whether the new point should be accepted or rejected, dependent on the criteria of the specific method

The end goal is then to sample from the most likely values of the parameters to generate a new, more likely distribution of the original parameters.  For this project, then, Bayesian Updating will produce new distributions for the three parameters that define the GEV distribution.

## Implementation

Implementation of this Bayesian Updating methodology is inspired by @storopoli2021bayesianjulia as well as @ceve543_2023.  Primarily, original inspiration for the utilization of the Turing package comes from @ceve543_2023 and plotting implementation from @storopoli2021bayesianjulia.

Again, the edits produced during Bayesian Updating change the storm surge distribution package rather than the code in HouseElevation.jl.  Consequently, changes can be found here in the template.qmd file.

The following code block demonstrates the creation of the model, using Turing, to sample from the posterior distribution of the parameters.  The model is defined as a function, `surge_model`, that takes in the data from the nearest gage.  The model then defines the prior distributions for the three parameters of the GEV distribution.  The model then loops through the data from the nearest gage, defining the likelihood of the data given the parameters.

1. Define the model as a function which inputs gage data

2. Define the prior distributions for the three parameters of the GEV distribution

3. Loop through the gage data, defining the likelihood of the data given the gage surge heights

```{julia}
#| output: false
#| echo: false

using Turing
using Statistics
using MCMCChains
using DynamicPPL
using AdvancedHMC
using DistributionsAD
using Bijectors
using Plots
using Distributions
using CairoMakie
using AlgebraOfGraphics
using CairoMakie 
```

```{julia}
@model function surge_model(y) #<1>
    μ ~ Normal(5, 1) #<2>
    σ ~ Exponential(1.25)
    ξ ~ Normal(0.1, 0.05)

    for i in eachindex(y)
        y[i] ~ GeneralizedExtremeValue(μ, σ, ξ) #<3>
    end
end;
```

Next, observational data is needed to update the distribution.  Given that the function predicts the maximum surge height for a given year, the data for which the distribution needs to be updated is the maximum surge height for the year.  This data is then input into the model to sample from the posterior distribution of the parameters. 

Given that storm surges are inherently spatial, the best data to update our parameter distributions is the data from the nearest gage.  For this particular project, the nearest gage is located in Galveston, Texas on Pier 21 [@noaa2024galveston].  I was able to download the time series information and isolate the highest storm surge for each month.  I then selected the highest storm surge for the year and saved it to the variable dataex.  The information from the past 10 years was then fed into my model.

```{julia}
#| output: false
#| echo: false

dataex = [3.97, 3.85, 4.71, 5.39, 3.92, 4.03, 4.03, 3.84, 3.94, 3.03, 3.11];
```

The following cell pulls in the last 10 years of observational data from the Galveston gage.  This data is then fed into the model to sample from the posterior distribution of the parameters.  The code uses a No U-Turn Sampler (e.g. NUTS) with 1,000 samples.

I have included the original output of the sampler below.

```{julia}
model = surge_model(dataex)
chain = sample(model, Turing.NUTS(), 1000)
```

## Validation

Validation for this Bayesian Updating is difficult as the true values of the parameters are unknown.  However, the updated distributions can be used to find an "expected value" of the storm surge.  If this value of storm surge seems reasonable given NOAA gage data, then the Bayesian Updating can be considered successful.

Julia allows users to find the expected value of the distribution using describe(chain) as discussed by @storopoli2021bayesianjulia.  In our project, the expected value of the storm surge distribution is 5.31 feet. This value, when compared to the NOAA gage, seems reasonable and lends credibility to the fact that our updating was successful.

```{julia}
#| output: false
#| echo: false
summaries, quantiles = describe(chain);
sum([idx * i for (i, idx) in enumerate(summaries[:, :mean])])
```

However, though this validation tells us that the Bayesian updating did in fact update our parameter distributions and produces expected surge values which are reasonable, this cannot definitively prove that these new distributions are correct.  There are no "true" parameter values to which we can compare, and we can only definitively show that these new distributions are plausible and possible given historical data.

# Results

First, when analyzing the results, both the sampling from the posterior as well as the new distributions of the parameters can be plotted.  Again, plotting guidance came from @storopoli2021bayesianjulia.  Note that the plots on the left show the sampler as it moves through the posterior space.  These sampler plots show good convergence, supporting the belief that the sampler is effecive.  Furthermore, the plots on the right show the new distributions of the parameters.

```{julia}
#| echo: false
#| output: false
params = names(chain, :parameters)
chain_mapping = mapping(params .=> "sample value") *
                mapping(; color=:chain => nonnumeric, row=dims(1) => renamer(params))
plt1 = AlgebraOfGraphics.data(chain) * mapping(:iteration) * chain_mapping * visual(Lines)
plt2 = AlgebraOfGraphics.data(chain) * chain_mapping * AlgebraOfGraphics.density()  

f = Figure(; resolution=(800, 600))
draw!(f[1, 1], plt1)
draw!(f[1, 2], plt2; axis=(; ylabel="density"))
display(f)
```

![]("ceve521newparameterdistributions.png")

These new distributions are more consistent with real, observed data from the Pier 21 gage on Galveston, TX.  In other words, for these new parameter distributions, the maximum surge we have seen at Pier 21 gage is more likely than if the original parameter distributions were true.  This supports the idea of the new, Bayesian updated distributions producing storm surge which is more accurate for our particular site and its physical location and geography.

Then, these new parameter distributions are used to create a new function which now draws a random storm surge given the information from the Bayesin updating.  It is defined by randomly selecting an index within the chain and pulling the three parameters at that index.  It would be incorrect to sample an independent draw from each parameter independently as the parameters are correlated.  The function is defined below.

```{julia}
function draw_surge_dist_bayesian()
    randindex = rand(1:length(chain))
    μ = chain[:μ][randindex]
    σ = chain[:σ][randindex]
    ξ = chain[:ξ][randindex]
    return GeneralizedExtremeValue(μ, σ, ξ)
end
```

The difference between the draws from the original GEV distribution and the draws from the Bayesian-updated GEV distribution can be seen below.  Note that the figure shows 10 random draws from the different functions.

```{julia}
#| echo: false
#| output: false
function draw_surge_distribution()
    μ = rand(Normal(5, 1))
    σ = rand(Exponential(1.25))
    ξ = rand(Normal(0.1, 0.05))
    return GeneralizedExtremeValue(μ, σ, ξ)
end

gev1 = draw_surge_distribution()
gevbayes = draw_surge_dist_bayesian()
x = range(0, 10, length=1000)
y = pdf(gev1, x)
plot1 = Plots.plot(x, y, label="Original GEV", lw=2, color=:red, legend=true, xlabel="Surge Height (m)", ylabel="Density")

for i = 1:10
    gevorigfor = draw_surge_distribution()
    y = pdf(gevorigfor, x)
    Plots.plot!(x, y, label="Original GEV", lw=2, color=:red, legend=false)
end

x = range(0, 10, length=1000)
y = pdf(gevbayes, x)
Plots.plot!(x, y, label="GEV Bayesian", lw=2)

for i = 1:10
    gevbayesfor = draw_surge_dist_bayesian()
    y = pdf(gevbayesfor, x)
    Plots.plot!(x, y, label="GEV Bayesian", lw=2, color=:blue)
end

plot1 = Plots.plot!(title="Original GEV Distributions vs. \n Bayesian GEV Distributions")
display(plot1)
```

![]("ceve521image2.png")

Note that the draws from the bayesian updated GEV distribtion are more concentrated and experience less variability between draws.  This represents a key tradeoff.  The Bayesian updated distribution is more accurate and consistent given the gage data on which it was trained.  This means that it is consistent with the spatial trends in that area, due both to spatial trends in climatology and nearby coastal geometry and bathymetry. However, it is updated only on limited historical data.  This means it may not generalize out as well to the future, and it might be less robust to any quick or extreme changes.  

This question is part of a larger question of balancing accuracy with historical data and availability to generalize well to other conditions.  In other words, it helps represent the balance between bias and variance in the model.  The Bayesian updated distribution is more biased towards the historical data, but it may have less variance in the future.  The original GEV distribution is less biased towards the historical data, but it may have more variance in the future.

In this way, this Bayesian updating is an important first step in including spatial trends in storm surge into our surge draw function.  However, future work must consider the balance between incorporating this spatial trend and representing historical data well with the need to generalize to future conditions.

Hypothetically, the Bayesian updating could be improved by adding an additional parameter which includes non-stationarity.  This parameter could represent the change in the distribution over time.  This would allow the model to update the distribution based on the most recent data, but also to consider how the distribution might change in the future.  This would help the model generalize better to future conditions.  On the other hand, this Bayesian updating could be improved by using Bayes' theorem to shift the distribution based on historical data and then adding more variance back in.

In general, it is a future area of research with signficant potential for exploration.

## Policy Search

We can also explore what happens when we use this new Bayesian-updated distributions in our policy search.

```{julia}
#| echo: false
#| output: false

using Revise
using HouseElevation

using CSV
using DataFrames
using DataFramesMeta
using Distributions
using LaTeXStrings
using Metaheuristics
using Plots
using Random
using Unitful

Plots.default(; margin=5Plots.mm)
```

Compared to the analysis in Lab 06, the difference between these codes is the inclusion of the Bayesian parameter distributions rather than the original distributions.

```{julia}
#| echo: false
#| output: false

slr_scenarios = let
    df = CSV.read("data/slr_oddo.csv", DataFrame)
    [Oddo17SLR(a, b, c, tstar, cstar) for (a, b, c, tstar, cstar) in eachrow(df)]
end

house = let
    haz_fl_dept = CSV.read("data/haz_fl_dept.csv", DataFrame) # read in the file
    desc = "two story, no basement, Structure"
    occ = "RES1"
    src = "USACE - Galveston"
    row = @rsubset(haz_fl_dept, :Description == desc, :Occupancy == occ, :Source == src)[1, :] 
    area = 881u"ft^2"
    height_above_gauge = 4*u"ft"
    House(row; area=area, height_above_gauge=height_above_gauge, value_usd=194_456)
end

p = ModelParams(; house=house, years=2024:2083)

function draw_surge_dist_bayesian()
    randindex = rand(1:length(chain))
    μ = chain[:μ][randindex]
    σ = chain[:σ][randindex]
    ξ = chain[:ξ][randindex]
    return GeneralizedExtremeValue(μ, σ, ξ)
end

function draw_discount_rate()
    draw = rand(Normal(0.04, 0.02))
    if draw < 0
        return 0.001
    else
        return draw
    end
end

N_SOW = 100
sows = [
    SOW(rand(slr_scenarios), draw_surge_dist_bayesian(), draw_discount_rate()) for
    _ in 1:N_SOW
] 
```

```{julia}
#| echo: false
#| output: false

Random.seed!(2024) ; #<1>
N_SOW = 100_000 ; #<2>
sows = [
    SOW(rand(slr_scenarios), draw_surge_dist_bayesian(), draw_discount_rate()) for _ in 1:N_SOW
    ] ;
N_SOW_opt = 300 ; #<3>
sows_opt = sows[1:N_SOW_opt] ;
```

```{julia}
#| echo: false
#| output: false

function objective_function_scalar(a::Float64)
    action = Action(a*u"ft") #<1>

    expected_values = [run_sim(action, sow, p) for sow in sows_opt] #<2>

    return -sum(expected_values) #<3>
end

function objective_function(a::Vector{Float64})
    action = Action(a[1]*u"ft") #<1>

    expected_values = [run_sim(action, sow, p) for sow in sows_opt] #<2>

    return -sum(expected_values) #<3>
end
```

```{julia}
#| echo: false
#| output: false

bounds = boxconstraints(; lb=0.0, ub=14.0) #<1>
options = Options(; time_limit=120.0) #<2>
algorithm = ECA(; options=options) #<3>
result = optimize(objective_function, bounds, algorithm) #<4>
``` 

```{julia}
#| echo: false
#| output: false

elevations = range(0; stop=14, length=100) #<1>
objective_values = [objective_function_scalar(e) for e in elevations] #<2>
Plots.plot(elevations, objective_values; xlabel="Elevation (ft)", ylabel="Objective function value", label="", title="Objective function for different elevations, 
Number of SOWs = $N_SOW_opt") #<3>
```

This code determines that the minimizing height is 8.30284 feet.  This is the height that minimizes the cost of damages and construction across all states of the world. 

On the other hand, the minimizer found with the original distribution function was 13.23 feet.  

Noteably, using gage data to include spatial trends within our storm surge analysis (e.g. to make the distributions more coherent with the physical location of the site) has a significant impact on the policy search.  The height that minimizes the cost of damages and construction is significantly lower when using the Bayesian-updated distribution.  This suggests that the Bayesian-updated distribution has shifted the distributions lower based on the gage data.

## Limitations

This result of lowering the recommended house elevation height, though it does show the impact of using Bayes' theorem to update the storm surge distribution, also suggests some limitations of this methodology. 

1. As mentioned above, this usage of Bayesian Updating represents a key tradeoff between performing well on historical data but failing to generalize well into the future.  There is a danger of overfitting to what we have seen in previous years, to the point of performing well when new conditions arise.  It is an area that must be considered, but again storm surge is inherently spatial, and we must have some way of representing the physical location.

2. The gage data itself to which the model was adapted is not ideal.  I had difficulty answering this question, but it seemed that NOAA was finding the maximum height but averaged across the study period.  In other words, it was smoothing the results.  This was extremely evident in a 1 hour time step as the surge was heavily averaged and, consequently, the highest peak was greatly decreased.  I combatted this by using the smallest time step, 6 minutes, but this still represents a significant smoothing.  This could have potentially, artifically lowered the storm surge maximums, causing the distributions to shift lower as we saw in the policy search.

3. Another limitation is the lack of a true validation set.  The Bayesian updating was validated by comparing the expected value of the distribution to the gage data, but this is not a true validation.  There is no true validation set to which we can compare the new distributions.  This means that we cannot definitively say that the new distributions are correct.  We can only say that they are plausible given the historical data.

4. Limited historical data also causes issues.  Firstly, it is difficult to access these storm surge maximums as NOAA limits the amount of data that can be exported at a time.  This means that, for each year, 12 individual spreadsheets had to be downloaded and sorted to find maximums.  In this way, the availability of the data made it difficult to pull many different years.  In addition, I did not want to go too far back because, if storm surge is non-stationary, the data from 20 years ago may not be representative of the data today.  The farther back I went, the increased risk of predicting to the wrong distribution.  For this reason, there is another important tradeoff here in including enough data to not be skewed to the most recent years, which could be quite random or unusual, while still avoiding problems with non-stationarity.

# Conclusions

The most important implication of this project is the inclusion of spatial trends in storm surge into the decision-support tool.  As discussed in the articles, factors like climatology of the storms themselves as well as bathymetry, topography, and coastal geometry impact storm surge.  This means that a singular storm surge distribution should likely not be used to represent surge along the entire US coast.  There are important distinctions that must be made.

This project used Bayesian Updating to make the generalized storm surge distribution more specific to the site of interest.  This was done by updating the parameters of the GEV distribution based on the nearest gage data, Pier 21 in Galveston.  The new parameter distributions produced expected surge values consistent with historical data, a validation that showed the results were at least plausible.  These new distributions were also used to reconduct the policy search analysis, showing a clear impact as they lowered the recommended elevation height.

This finding was particularly interesting.  It is possible that this finding is representative of the fact that this gage and the site of interest are on the back side of Galveston island rather than the side exposed to direct surge.  This might mean that the surge experienced here is tapered compared to the South side.  However, it is also possible that these distribtions were artifically lowered due to the averaging that occurs in NOAA gage data.  This is definitely an area for further work.

There are additional limitations to this methodology.  Primarily, it raises important questions of the relative need to perform well on historical data and reasonably fit nearby spatial data versus adjusting and adapting well to the future or new conditions.  Furthermore, the lack of a true validation set and the limited historical data available are important limitations to consider.

In general, this project starts the exploration of including spatial trends in storm surge into the decision-support tool.  It is an important first step in improving the accuracy of the storm surge distribution used in the tool.  However, there are important tradeoffs to consider in this methodology, and future work must consider these tradeoffs and potential improvements.

# References

:::{#refs}
:::